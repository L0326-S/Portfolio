{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs75M22Q942GvZLQFvCh4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L0326-S/Portfolio/blob/main/%EA%B0%95%EC%A7%80%EC%88%98_202011842_%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98%EA%B8%B0%EB%A5%BC_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%8A%A4%ED%8C%B8_%EB%A9%94%EC%9D%BC_%EB%B6%84%EB%A5%98_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트명: 나이브 베이즈 분류기를 활용한 스팸 메일 분류 프로그램\n",
        "- 학과(부)/학번/성명: 통계학과/2020-11842/강지수\n"
      ],
      "metadata": {
        "id": "ZiCZFHd2TUQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.프로젝트 요약(공백포함 500글자 이내)\n",
        "\n",
        "\n",
        "*   **목적**: 이메일 텍스트를 분석하여 스팸 메일과 정상(햄) 메일을 구분하는 나이브 베이즈 분류 모델을 구현하고 평가한다.\n",
        "*   **내용**: Kaggle에서 제공되는 이메일 스팸 분류 데이터셋을 활용한다. 데이터를 로드하고, 이메일 텍스트에 대한 전처리(소문자 변환, 특수문자 제거, 토큰화, 불용어 제거)를 수행한다. 전처리된 데이터를 바탕으로 나이브 베이즈 분류기 클래스를 직접 구현하여 학습시킨다. 학습된 모델의 성능을 정확도, 정밀도, 재현율, F1 점수 지표로 평가한다. 최종적으로 사용자가 직접 이메일 내용을 입력하면 모델이 스팸 여부를 예측하고, 분류에 기여한 단어를 시각화하여 결과를 보여준다.\n",
        "*   **결론**: 구현된 나이브 베이즈 분류기는 테스트 데이터셋에서 높은 정확도와 F1 점수를 기록하며 스팸 분류에 효과적으로 작동함을 확인하였다. 특정 키워드들이 스팸 또는 정상 분류에 기여하는 정도를 시각적으로 확인함으로써 모델의 해석 가능성을 높였다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PtnayE-bmQAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.실행환경\n",
        "* OS: Linux (Google Colab 환경)\n",
        "* 파이썬 툴: Google Colab\n",
        "* 설치가 필요한 모듈: kagglehub (Kaggle 데이터셋 다운로드를 위해 사용)"
      ],
      "metadata": {
        "id": "FeAFt5lTSZpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.사용된 문제/데이터 출처\n",
        "- 데이터 출처: 이메일 스팸 분류 데이터셋: Kaggle \"Spam Email Classification Dataset\"\n",
        " -\n",
        "https://www.kaggle.com/datasets/purusinghvi/email-spam-classification-dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1UzHmvUv1WQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.전체코드"
      ],
      "metadata": {
        "id": "ZYZFDuriUHvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Sample\n",
        "```\n",
        "Hello, I've attached the meeting minutes. Let's discuss them again during the meeting next Monday at 10 AM. Please feel free to contact me if you have any questions. Have a great day!\n",
        "Free Lottery Win! Your number has been selected. Click here to verify your personal information and receive your huge prize money. Hurry, this is a one-time opportunity!\n",
        "exit\n",
        "```\n",
        "Output Sample\n",
        "```\n",
        "Requirement already satisfied: kagglehub==0.3.12 in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
        "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (24.2)\n",
        "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (6.0.2)\n",
        "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (2.32.3)\n",
        "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (4.67.1)\n",
        "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (3.4.2)\n",
        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (3.10)\n",
        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (2.4.0)\n",
        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (2025.4.26)\n",
        "--- NLTK 데이터 다운로드 시작 ---\n",
        "--- NLTK 데이터 다운로드 완료 ---\n",
        "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!\n",
        "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
        "[nltk_data]   Package stopwords is already up-to-date!\n",
        "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
        "[nltk_data]   Package wordnet is already up-to-date!\n",
        "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
        "[nltk_data]   Package punkt_tab is already up-to-date!\n",
        "Path to dataset files: /kaggle/input/email-spam-classification-da\\taset\n",
        "/kaggle/input/email-spam-classification-dataset/combined_data.csv\n",
        "\n",
        "==================================================\n",
        "       이메일 스팸 분류기 - 사용자 입력 모드\n",
        "==================================================\n",
        "이메일 내용을 입력해주세요. (입력을 마치려면 '끝' 또는 'exit'을 입력하세요)\n",
        "\n",
        "# 이메일 내용 입력: Hello, I've attached the meeting minutes. Let's discuss them again during the meeting next Monday at 10 AM. Please feel free to contact me if you have any questions. Have a great day!\n",
        "\n",
        "입력하신 이메일 내용:\n",
        "Hello, I've attached the meeting minutes. Let's discuss them again during the meeting next Monday at 10 AM. Please feel free to contact me if you have any questions. Have a great day!\n",
        "\n",
        "이 이메일은 '정상' 메일로 분류되었습니다.\n",
        "스팸 로그 확률: -137.21\n",
        "정상 로그 확률: -131.29\n",
        "\n",
        "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n",
        "hello , i 've attached the meeting minutes . let 's discuss them again during the meeting next monday at 10 am . please feel free to contact me if you have any questions . have a great day !\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "이메일 내용 입력: Free Lottery Win! Your number has been selected. Click here to verify your personal information and receive your huge prize money. Hurry, this is a one-time opportunity!\n",
        "\n",
        "입력하신 이메일 내용:\n",
        "Free Lottery Win! Your number has been selected. Click here to verify your personal information and receive your huge prize money. Hurry, this is a one-time opportunity!\n",
        "\n",
        "이 이메일은 '스팸' 메일로 분류되었습니다.\n",
        "스팸 로그 확률: -130.36\n",
        "정상 로그 확률: -149.56\n",
        "\n",
        "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n",
        "free lottery win ! your number has been selected . click here to verify your personal information and receive your huge prize money . hurry , this is a one-time opportunity !\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "이메일 내용 입력: exit\n",
        "프로그램을 종료합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "ldK4YQV095Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 준비작업: 필요한 라이브러리 설치 (Google Colab에 kagglehub가 기본 설치되어 있지 않은 경우 필요)\n",
        "!pip install kagglehub==0.3.12\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "# %%\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from IPython.display import HTML, display\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# %%\n",
        "print(\"--- NLTK 데이터 다운로드 시작 ---\")\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"--- NLTK 데이터 다운로드 완료 ---\")\n",
        "except Exception as e:\n",
        "    print(f\"NLTK 다운로드 중 오류 발생: {e}\")\n",
        "    print(\"네트워크 연결을 확인하거나 Google Colab 런타임 (런타임 > 런타임 다시 시작) 후 다시 시도해 보세요.\")\n",
        "# %%\n",
        "\n",
        "\n",
        "# --- 1. 데이터 로딩 ---\n",
        "\n",
        "# Download latest version\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"purusinghvi/email-spam-classification-dataset\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "except Exception as e:\n",
        "    print(f\"Kaggle 데이터셋 다운로드 중 오류 발생: {e}\")\n",
        "    print(\"Kaggle 인증이 필요할 수 있습니다. (Colab 메뉴: Runtime -> Change runtime type -> Command palette 검색 후 'kaggle' 입력하여 API 토큰 설정)\")\n",
        "    raise # 오류 발생 시 프로그램 중단\n",
        "\n",
        "# --- 디렉토리 내용 확인 ---\n",
        "try:\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for name in files:\n",
        "            print(os.path.join(root, name))\n",
        "        for name in dirs:\n",
        "            print(os.path.join(root, name) + \"/\")\n",
        "except Exception as e:\n",
        "     print(f\"데이터 디렉토리 확인 중 오류 발생: {e}\")\n",
        "\n",
        "\n",
        "# 데이터셋 파일 경로 설정: 'combined_data.csv' 파일 사용\n",
        "data_file_path = os.path.join(path, 'combined_data.csv')\n",
        "\n",
        "# 데이터 로드\n",
        "try:\n",
        "    df = pd.read_csv(data_file_path, encoding='latin-1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"UTF-8 디코딩 오류 발생. 다른 인코딩으로 시도합니다.\")\n",
        "    df = pd.read_csv(data_file_path, encoding='ISO-8859-1')\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: '{data_file_path}' 파일을 찾을 수 없습니다. 경로를 다시 확인해주세요.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"데이터 로드 중 알 수 없는 오류 발생: {e}\")\n",
        "    raise\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "# --- 2. 데이터 전처리 ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    이메일 텍스트를 전처리하는 함수:\n",
        "    - 소문자 변환\n",
        "    - 특수문자 및 숫자 제거\n",
        "    - 토큰화\n",
        "    - 불용어 제거\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# 'text' 컬럼에 전처리 함수 적용\n",
        "df['processed_message'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "# --- 3. 나이브 베이즈 분류기 구현 ---\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.spam_words = defaultdict(int)\n",
        "        self.ham_words = defaultdict(int)\n",
        "        self.spam_count = 0\n",
        "        self.ham_count = 0\n",
        "        self.total_spam_words = 0\n",
        "        self.total_ham_words = 0\n",
        "        self.vocab_size = 0\n",
        "        self.prior_spam = 0.0\n",
        "        self.prior_ham = 0.0\n",
        "        self.all_words = set() # 전체 단어 집합을 저장하기 위한 set 추가\n",
        "\n",
        "    def train(self, messages, labels):\n",
        "        for i, message_tokens in enumerate(messages):\n",
        "            if labels[i] == 1: # Spam (1)\n",
        "                self.spam_count += 1\n",
        "                for word in message_tokens:\n",
        "                    self.spam_words[word] += 1\n",
        "                    self.total_spam_words += 1\n",
        "                    self.all_words.add(word) # 전체 단어 집합에 추가\n",
        "            else: # Ham (0)\n",
        "                self.ham_count += 1\n",
        "                for word in message_tokens:\n",
        "                    self.ham_words[word] += 1\n",
        "                    self.total_ham_words += 1\n",
        "                    self.all_words.add(word) # 전체 단어 집합에 추가\n",
        "\n",
        "        self.vocab_size = len(self.all_words) # 학습 데이터의 전체 단어 집합 크기 사용\n",
        "        total_emails = self.spam_count + self.ham_count\n",
        "        self.prior_spam = self.spam_count / total_emails\n",
        "        self.prior_ham = self.ham_count / total_emails\n",
        "\n",
        "    def calculate_likelihood(self, word, class_type):\n",
        "        # 라플라스 스무딩 적용: 알려지지 않은 단어에 대해 0 확률이 되는 것을 방지\n",
        "        if class_type == 'spam':\n",
        "            # 학습 데이터의 전체 단어 집합 크기를 사용하여 스무딩 적용\n",
        "            return (self.spam_words.get(word, 0) + 1) / (self.total_spam_words + self.vocab_size)\n",
        "        else: # ham\n",
        "             # 학습 데이터의 전체 단어 집합 크기를 사용하여 스무딩 적용\n",
        "            return (self.ham_words.get(word, 0) + 1) / (self.total_ham_words + self.vocab_size)\n",
        "\n",
        "    def predict(self, message_tokens):\n",
        "        # 로그 확률 계산 시 초기값을 priors의 로그값으로 설정\n",
        "        prob_spam = np.log(self.prior_spam)\n",
        "        prob_ham = np.log(self.prior_ham)\n",
        "\n",
        "        for word in message_tokens:\n",
        "            # 알려지지 않은 단어에 대한 처리 (스무딩으로 이미 처리됨)\n",
        "            # 각 단어에 대한 로그 가능도를 더함\n",
        "            prob_spam += np.log(self.calculate_likelihood(word, 'spam'))\n",
        "            prob_ham += np.log(self.calculate_likelihood(word, 'ham'))\n",
        "\n",
        "        return 1 if prob_spam > prob_ham else 0, prob_spam, prob_ham\n",
        "\n",
        "# 데이터 분리 (훈련 세트와 테스트 세트)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['processed_message'], df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 분류기 인스턴스 생성 및 학습\n",
        "classifier = NaiveBayesClassifier()\n",
        "classifier.train(X_train.tolist(), y_train.tolist())\n",
        "\n",
        "# --- 4. 성능 평가 ---\n",
        "\n",
        "y_pred = []\n",
        "# 테스트 세트의 메시지 토큰에 대해 예측 수행\n",
        "for msg_tokens in X_test.tolist():\n",
        "    pred, _, _ = classifier.predict(msg_tokens)\n",
        "    y_pred.append(pred)\n",
        "\n",
        "# 성능 지표 계산\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# print(f\"Accuracy (정확도): {accuracy:.4f}\")\n",
        "# print(f\"Precision (정밀도): {precision:.4f}\")\n",
        "# print(f\"Recall (재현율): {recall:.4f}\")\n",
        "# print(f\"F1-Score (F1 점수): {f1:.4f}\")\n",
        "# %%\n",
        "\n",
        "\n",
        "# --- 5. 모델 해석 및 시각화 함수 ---\n",
        "def visualize_email_classification(classifier_obj, email_text_input):\n",
        "    \"\"\"\n",
        "    사용자 입력 이메일을 분류하고, 분류에 기여한 키워드를 시각화하는 함수.\n",
        "    \"\"\"\n",
        "    print(f\"\\n입력하신 이메일 내용:\\n{email_text_input}\\n\")\n",
        "\n",
        "    # 원본 텍스트를 기준으로 토큰화 (시각화를 위해)\n",
        "    original_tokens = word_tokenize(str(email_text_input).lower())\n",
        "    # 전처리된 토큰 (모델 예측에 사용)\n",
        "    processed_tokens = preprocess_text(email_text_input)\n",
        "\n",
        "    if not processed_tokens:\n",
        "        print(\"전처리된 메시지에 분류 가능한 단어가 없어 분류할 수 없습니다. 이메일 내용을 확인해주세요.\")\n",
        "        # 시각화는 건너뛰지만, 원본 텍스트는 표시\n",
        "        display(HTML(\" \".join(original_tokens)))\n",
        "        return\n",
        "\n",
        "    # 모델 예측 수행\n",
        "    prediction, prob_spam, prob_ham = classifier_obj.predict(processed_tokens)\n",
        "    classification_result = \"스팸\" if prediction == 1 else \"정상\"\n",
        "\n",
        "    print(f\"이 이메일은 '{classification_result}' 메일로 분류되었습니다.\")\n",
        "    print(f\"스팸 로그 확률: {prob_spam:.2f}\")\n",
        "    print(f\"정상 로그 확률: {prob_ham:.2f}\\n\")\n",
        "\n",
        "    highlighted_text_parts = []\n",
        "    # 원본 토큰을 순회하며 시각화 적용\n",
        "    for word in original_tokens:\n",
        "        # 시각화에 사용할 단어는 특수문자 제거 후 소문자 변환하여 처리\n",
        "        cleaned_word = re.sub(r'[^a-z]', '', word).lower()\n",
        "\n",
        "        # 전처리 후 남은 단어에 대해서만 가능도 계산 및 색상 적용\n",
        "        if cleaned_word in classifier_obj.all_words: # 학습 데이터의 전체 단어 집합에 포함된 경우만 고려\n",
        "            # 로그 가능도 계산\n",
        "            spam_likelihood_log = np.log(classifier_obj.calculate_likelihood(cleaned_word, 'spam'))\n",
        "            ham_likelihood_log = np.log(classifier_obj.calculate_likelihood(cleaned_word, 'ham'))\n",
        "\n",
        "            # 로그 가능도 차이를 기준으로 색상 결정\n",
        "            # 차이가 클수록 해당 클래스에 강하게 기여한다고 판단\n",
        "            # np.log(1.5)는 임계값으로, 이 값보다 차이가 커야 강한 기여로 판단\n",
        "            log_likelihood_ratio = spam_likelihood_log - ham_likelihood_log\n",
        "\n",
        "            if log_likelihood_ratio > np.log(1.5): # 스팸에 강하게 기여\n",
        "                highlighted_text_parts.append(f'<span style=\"color:red; font-weight:bold;\">{word}</span>')\n",
        "            elif log_likelihood_ratio < -np.log(1.5): # 정상에 강하게 기여\n",
        "                 highlighted_text_parts.append(f'<span style=\"color:blue; font-weight:bold;\">{word}</span>')\n",
        "            else: # 큰 차이 없는 경우\n",
        "                highlighted_text_parts.append(word)\n",
        "        else: # 학습 데이터에 없는 단어는 색상 적용 없이 추가\n",
        "            highlighted_text_parts.append(word)\n",
        "\n",
        "    print(\"이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\")\n",
        "    display(HTML(\" \".join(highlighted_text_parts)))\n",
        "# %%\n",
        "\n",
        "\n",
        "# --- 사용자 입력 루프 ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"       이메일 스팸 분류기 - 사용자 입력 모드\")\n",
        "print(\"=\"*50)\n",
        "print(\"이메일 내용을 입력해주세요. (입력을 마치려면 '끝' 또는 'exit'을 입력하세요)\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"이메일 내용 입력: \")\n",
        "    if user_input.lower() in ['끝', 'exit']:\n",
        "        print(\"프로그램을 종료합니다.\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        # 입력된 이메일 내용을 시각화 함수에 전달\n",
        "        visualize_email_classification(classifier, user_input)\n",
        "    except Exception as e:\n",
        "        print(f\"처리 중 오류 발생: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "IfUFVQbmUK7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a1eac19-66cb-46b9-c857-a2077f7d33eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub==0.3.12 in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.3.12) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.3.12) (2025.4.26)\n",
            "--- NLTK 데이터 다운로드 시작 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NLTK 데이터 다운로드 완료 ---\n",
            "Path to dataset files: /kaggle/input/email-spam-classification-dataset\n",
            "/kaggle/input/email-spam-classification-dataset/combined_data.csv\n",
            "\n",
            "==================================================\n",
            "       이메일 스팸 분류기 - 사용자 입력 모드\n",
            "==================================================\n",
            "이메일 내용을 입력해주세요. (입력을 마치려면 '끝' 또는 'exit'을 입력하세요)\n",
            "\n",
            "이메일 내용 입력: Hello, I've attached the meeting minutes. Let's discuss them again during the meeting next Monday at 10 AM. Please feel free to contact me if you have any questions. Have a great day!\n",
            "\n",
            "입력하신 이메일 내용:\n",
            "Hello, I've attached the meeting minutes. Let's discuss them again during the meeting next Monday at 10 AM. Please feel free to contact me if you have any questions. Have a great day!\n",
            "\n",
            "이 이메일은 '정상' 메일로 분류되었습니다.\n",
            "스팸 로그 확률: -137.21\n",
            "정상 로그 확률: -131.29\n",
            "\n",
            "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "hello , i 've <span style=\"color:blue; font-weight:bold;\">attached</span> the <span style=\"color:blue; font-weight:bold;\">meeting</span> <span style=\"color:red; font-weight:bold;\">minutes</span> . let 's <span style=\"color:blue; font-weight:bold;\">discuss</span> them again during the <span style=\"color:blue; font-weight:bold;\">meeting</span> next <span style=\"color:blue; font-weight:bold;\">monday</span> at 10 am . please <span style=\"color:red; font-weight:bold;\">feel</span> free to <span style=\"color:red; font-weight:bold;\">contact</span> me if you have any <span style=\"color:blue; font-weight:bold;\">questions</span> . have a <span style=\"color:red; font-weight:bold;\">great</span> <span style=\"color:red; font-weight:bold;\">day</span> !"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "이메일 내용 입력: Free Lottery Win! Your number has been selected. Click here to verify your personal information and receive your huge prize money. Hurry, this is a one-time opportunity!\n",
            "\n",
            "입력하신 이메일 내용:\n",
            "Free Lottery Win! Your number has been selected. Click here to verify your personal information and receive your huge prize money. Hurry, this is a one-time opportunity!\n",
            "\n",
            "이 이메일은 '스팸' 메일로 분류되었습니다.\n",
            "스팸 로그 확률: -130.36\n",
            "정상 로그 확률: -149.56\n",
            "\n",
            "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "free <span style=\"color:red; font-weight:bold;\">lottery</span> win ! your number has been <span style=\"color:red; font-weight:bold;\">selected</span> . <span style=\"color:red; font-weight:bold;\">click</span> here to verify your <span style=\"color:red; font-weight:bold;\">personal</span> <span style=\"color:red; font-weight:bold;\">information</span> and <span style=\"color:red; font-weight:bold;\">receive</span> your <span style=\"color:red; font-weight:bold;\">huge</span> <span style=\"color:red; font-weight:bold;\">prize</span> <span style=\"color:red; font-weight:bold;\">money</span> . <span style=\"color:red; font-weight:bold;\">hurry</span> , this is a one-time <span style=\"color:red; font-weight:bold;\">opportunity</span> !"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "이메일 내용 입력: exit\n",
            "프로그램을 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.부분코드 작성요령\n",
        "\n",
        "프로젝트 요약 및 각 코드 블록의 설명 문구 작성에 생성형 AI(Gemini)의 도움을 받아 내용을 다듬었습니다."
      ],
      "metadata": {
        "id": "I9uLuuwXULv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-1. 환경 설정 및 데이터 로딩\n",
        "**설명**: 이 부분에서는 필요한 라이브러리를 임포트하고, NLTK 데이터를 다운로드하며, Kaggle에서 스팸 분류 데이터셋을 다운로드하여 pandas DataFrame으로 로드합니다. Kagglehub를 사용하여 데이터셋을 로컬 환경에 다운로드하는 과정을 포함합니다. 데이터 로드 시 발생할 수 있는 인코딩 오류를 처리하는 예외 처리를 추가했습니다.\n",
        "\n",
        "**생성형 AI 활용 내역**:\n",
        "* 자연어 처리 환경 설정: NLTK 라이브러리 설치 및 필요한 데이터(punkt, stopwords, wordnet 등)를 다운로드하고 설정하는 방법에 대한 안내를 받았습니다.\n",
        "*  데이터 불러오기 및 환경 설정: Kaggle 데이터셋을 Google Colab 환경에서 다운로드하고 pandas DataFrame으로 로드하는 방법, 특히 `kagglehub` 사용법 및 발생 가능한 파일 경로, 인코딩 문제 해결에 대한 가이드라인을 얻었습니다.\n",
        "\n",
        "이러한 도움을 통해 프로젝트 초기 환경 설정 및 데이터 준비 단계를 효율적으로 진행할 수 있었습니다.\n",
        "\n",
        "**구현 방법**:\n",
        "* !pip install kagglehub==0.3.12 명령어를 사용하여 kagglehub 라이브러리를 설치합니다.\n",
        "* Colab 환경에서는 대부분의 필요한 라이브러리가 이미 설치되어 있지만, kagglehub는 별도로 설치해야 할 수 있습니다.\n",
        "import 문을 통해 필요한 모듈들을 불러옵니다.\n",
        "* nltk.download()를 사용하여 텍스트 처리에 필요한 NLTK 데이터를 다운로드합니다.\n",
        "* kagglehub.dataset_download() 함수를 사용하여 지정된 Kaggle 데이터셋을 다운로드하고, 다운로드 경로를 확인합니다.\n",
        "* os.walk()를 사용하여 다운로드된 디렉토리의 파일 목록을 출력하여 데이터셋 파일의 존재 여부와 이름을 확인합니다.\n",
        "* pd.read_csv()를 사용하여 'combined_data.csv' 파일을 DataFrame으로 로드합니다. 다양한 인코딩으로 시도하여 오류를 방지합니다.\n",
        "* 로드된 데이터의 처음 몇 행 (.head())과 데이터 정보 (.info())를 출력하여 데이터 구조를 확인합니다."
      ],
      "metadata": {
        "id": "kvWSza3kVXCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 준비작업: 필요한 라이브러리 설치 (Google Colab에 kagglehub가 기본 설치되어 있지 않은 경우 필요)\n",
        "!pip install kagglehub==0.2.6\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from IPython.display import HTML, display\n",
        "import os\n",
        "import kagglehub\n",
        "# %%\n",
        "print(\"--- NLTK 데이터 다운로드 시작 ---\")\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    # nltk.download('punkt_tab') # 이 모듈은 불필요할 수 있습니다.\n",
        "    print(\"--- NLTK 데이터 다운로드 완료 ---\")\n",
        "except Exception as e:\n",
        "    print(f\"NLTK 다운로드 중 오류 발생: {e}\")\n",
        "    print(\"네트워크 연결을 확인하거나 Google Colab 런타임 (런타임 > 런타임 다시 시작) 후 다시 시도해 보세요.\")\n",
        "# %%\n",
        "# --- 1. 데이터 로딩 ---\n",
        "print(\"\\n--- 1. 데이터 로딩 ---\")\n",
        "\n",
        "# Download latest version\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"purusinghvi/email-spam-classification-dataset\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "except Exception as e:\n",
        "    print(f\"Kaggle 데이터셋 다운로드 중 오류 발생: {e}\")\n",
        "    print(\"Kaggle 인증이 필요할 수 있습니다. (Colab 메뉴: Runtime -> Change runtime type -> Command palette 검색 후 'kaggle' 입력하여 API 토큰 설정)\")\n",
        "    raise # 오류 발생 시 프로그램 중단\n",
        "\n",
        "# --- 디렉토리 내용 확인 ---\n",
        "print(\"\\n--- 디렉토리 내용 확인 ---\")\n",
        "try:\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for name in files:\n",
        "            print(os.path.join(root, name))\n",
        "        for name in dirs:\n",
        "            print(os.path.join(root, name) + \"/\")\n",
        "except Exception as e:\n",
        "     print(f\"데이터 디렉토리 확인 중 오류 발생: {e}\")\n",
        "\n",
        "print(\"------------------------\\n\")\n",
        "\n",
        "# 데이터셋 파일 경로 설정: 'combined_data.csv' 파일 사용\n",
        "data_file_path = os.path.join(path, 'combined_data.csv')\n",
        "\n",
        "# 데이터 로드\n",
        "try:\n",
        "    df = pd.read_csv(data_file_path, encoding='latin-1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"UTF-8 디코딩 오류 발생. 다른 인코딩으로 시도합니다.\")\n",
        "    df = pd.read_csv(data_file_path, encoding='ISO-8859-1')\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: '{data_file_path}' 파일을 찾을 수 없습니다. 경로를 다시 확인해주세요.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"데이터 로드 중 알 수 없는 오류 발생: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "print(\"원본 데이터셋 헤드:\")\n",
        "print(df.head())\n",
        "print(\"\\n원본 데이터셋 정보:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n데이터셋 컬럼 확인 (변경 없음):\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "DJfqu2zAUiO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dab223b-171c-4b36-8676-fc390bd34906"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub==0.2.6 in /usr/local/lib/python3.11/dist-packages (0.2.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.2.6) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.2.6) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub==0.2.6) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.2.6) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.2.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.2.6) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub==0.2.6) (2025.4.26)\n",
            "--- NLTK 데이터 다운로드 시작 ---\n",
            "--- NLTK 데이터 다운로드 완료 ---\n",
            "\n",
            "--- 1. 데이터 로딩 ---\n",
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/purusinghvi/email-spam-classification-dataset/versions/1\n",
            "\n",
            "--- 디렉토리 내용 확인 ---\n",
            "/root/.cache/kagglehub/datasets/purusinghvi/email-spam-classification-dataset/versions/1/combined_data.csv\n",
            "------------------------\n",
            "\n",
            "원본 데이터셋 헤드:\n",
            "   label                                               text\n",
            "0      1  ounce feather bowl hummingbird opec moment ala...\n",
            "1      1  wulvob get your medircations online qnb ikud v...\n",
            "2      0   computer connection from cnn com wednesday es...\n",
            "3      1  university degree obtain a prosperous future m...\n",
            "4      0  thanks for all your answers guys i know i shou...\n",
            "\n",
            "원본 데이터셋 정보:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 83448 entries, 0 to 83447\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   label   83448 non-null  int64 \n",
            " 1   text    83448 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "\n",
            "데이터셋 컬럼 확인 (변경 없음):\n",
            "   label                                               text\n",
            "0      1  ounce feather bowl hummingbird opec moment ala...\n",
            "1      1  wulvob get your medircations online qnb ikud v...\n",
            "2      0   computer connection from cnn com wednesday es...\n",
            "3      1  university degree obtain a prosperous future m...\n",
            "4      0  thanks for all your answers guys i know i shou...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-2. 데이터 전처리\n",
        "**설명**: 이 부분에서는 나이브 베이즈 분류 모델의 성능 향상을 위해 이메일 텍스트 데이터에 대한 전처리를 수행합니다. 전처리는 텍스트를 표준화하고 노이즈를 제거하는 과정을 포함합니다.\n",
        "\n",
        "**생성형 AI 활용 내역**:\n",
        "\n",
        "* 텍스트 전처리 파이프라인 구축: 이메일 텍스트 데이터에 적용할 표준적인 전처리 단계(소문자 변환, 특수문자 제거, 토큰화, 불용어 제거)를 구성하는 방법에 대한 조언을 얻었습니다\n",
        "\n",
        "**구현 방법**:\n",
        "\n",
        "* preprocess_text 함수를 정의합니다. 이 함수는 다음 단계를 수행합니다.\n",
        " * 입력된 텍스트를 모두 소문자로 변환합니다.\n",
        " *정규 표현식을 사용하여 알파벳 문자와 공백을 제외한 모든 특수문자 및 숫자를 제거합니다.\n",
        " *nltk.word_tokenize를 사용하여 텍스트를 개별 단어(토큰)로 분리합니다.\n",
        " *nltk.corpus.stopwords에서 영어 불용어 목록을 가져와 불용어에 해당하는 토큰과 길이가 1 이하인 토큰을 제거합니다.\n",
        " *전처리된 토큰 리스트를 반환합니다.\n",
        "*df['text'] 컬럼에 preprocess_text 함수를 적용하고, 그 결과를 새로운 컬럼 df['processed_message']에 저장합니다.\n",
        "*전처리된 메시지의 예시를 출력하여 결과를 확인합니다."
      ],
      "metadata": {
        "id": "TlFua-EyUjPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- 2. 데이터 전처리 ---\n",
        "print(\"\\n--- 2. 데이터 전처리 ---\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    이메일 텍스트를 전처리하는 함수:\n",
        "    - 소문자 변환\n",
        "    - 특수문자 및 숫자 제거\n",
        "    - 토큰화\n",
        "    - 불용어 제거\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# 'text' 컬럼에 전처리 함수 적용\n",
        "df['processed_message'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\n전처리된 메시지 예시:\")\n",
        "print(df['processed_message'].head())"
      ],
      "metadata": {
        "id": "lkVSiiSjUmBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3da09d-90d5-44d0-ee02-29b03b6c4449"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. 데이터 전처리 ---\n",
            "\n",
            "전처리된 메시지 예시:\n",
            "0    [ounce, feather, bowl, hummingbird, opec, mome...\n",
            "1    [wulvob, get, medircations, online, qnb, ikud,...\n",
            "2    [computer, connection, cnn, com, wednesday, es...\n",
            "3    [university, degree, obtain, prosperous, futur...\n",
            "4    [thanks, answers, guys, know, checked, rsync, ...\n",
            "Name: processed_message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-3. 나이브 베이즈 분류기 구현 및 학습\n",
        "**설명**: 이 부분에서는 나이브 베이즈 분류 알고리즘을 파이썬 클래스로 직접 구현하고, 전처리된 데이터를 사용하여 모델을 학습시킵니다. 클래스는 각 클래스(스팸/정상)별 단어 빈도, 클래스 확률, 그리고 예측 함수를 포함합니다. 라플라스 스무딩을 적용하여 알려지지 않은 단어의 확률이 0이 되는 것을 방지합니다.\n",
        "\n",
        "**생성형 AI 활용 내역**:\n",
        "* 파이썬 클래스 구조: 나이브 베이즈 분류기를 파이썬 클래스로 구현하기 위한 구조 설계, 필요한 속성(spam_words, ham_words, spam_count 등) 및 메서드(train, calculate_likelihood, predict) 구성에 대한 아이디어를 얻었습니다.\n",
        "* 라플라스 스무딩 구현: 학습 데이터에 나타나지 않은 단어의 확률을 0이 되지 않도록 방지하는 라플라스 스무딩(Add-1 smoothing)을 구현하는 방법에 대한 상세한 설명을 받았습니다. 특히, 단어 집합 크기(vocab_size)를 스무딩에 활용하는 방법에 대한 가이드라인이 포함되었습니다.\n",
        "\n",
        "**구현 방법**:\n",
        "\n",
        "* NaiveBayesClassifier 클래스를 정의합니다.\n",
        " * __init__ 메서드에서 스팸 및 정상 메일의 단어 빈도, 총 메일 수, 총 단어 수, 단어 사전 크기, 클래스 사전 확률을 저장할 변수를 초기화합니다.\n",
        " * train 메서드에서는 훈련 데이터를 사용하여 각 클래스별 단어 빈도와 총 단어 수를 계산하고, 단어 사전(vocabulary) 크기를 구합니다. 마지막으로 스팸 및 정상 메일의 사전 확률(prior probability)을 계산합니다.\n",
        " * calculate_likelihood 메서드는 특정 단어가 주어진 클래스(스팸 또는 정상)에 속할 확률을 계산합니다. 라플라스 스무딩 (+1)을 적용하여 0 빈도 단어 문제를 해결합니다. 학습 데이터의 전체 단어 집합 크기를 사용하여 스무딩을 적용합니다.\n",
        " *predict 메서드는 입력된 메시지의 토큰들을 사용하여 스팸일 확률과 정상일 확률을 계산합니다. 로그 확률을 사용하여 언더플로우를 방지하고, 두 확률을 비교하여 최종 분류 결과를 결정합니다.\n",
        "* 데이터를 훈련 세트와 테스트 세트로 분리합니다 (train_test_split).\n",
        "* NaiveBayesClassifier 클래스의 인스턴스를 생성하고, 훈련 데이터로 모델을 학습시킵니다.\n",
        "* 학습된 모델의 주요 통계 (스팸/정상 메일 수, 단어 사전 크기)를 출력합니다."
      ],
      "metadata": {
        "id": "EnOpokoBUpXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- 3. 나이브 베이즈 분류기 구현 ---\n",
        "print(\"\\n--- 3. 나이브 베이즈 분류기 구현 ---\")\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.spam_words = defaultdict(int)\n",
        "        self.ham_words = defaultdict(int)\n",
        "        self.spam_count = 0\n",
        "        self.ham_count = 0\n",
        "        self.total_spam_words = 0\n",
        "        self.total_ham_words = 0\n",
        "        self.vocab_size = 0\n",
        "        self.prior_spam = 0.0\n",
        "        self.prior_ham = 0.0\n",
        "        self.all_words = set() # 전체 단어 집합을 저장하기 위한 set 추가\n",
        "\n",
        "    def train(self, messages, labels):\n",
        "        for i, message_tokens in enumerate(messages):\n",
        "            if labels[i] == 1: # Spam (1)\n",
        "                self.spam_count += 1\n",
        "                for word in message_tokens:\n",
        "                    self.spam_words[word] += 1\n",
        "                    self.total_spam_words += 1\n",
        "                    self.all_words.add(word) # 전체 단어 집합에 추가\n",
        "            else: # Ham (0)\n",
        "                self.ham_count += 1\n",
        "                for word in message_tokens:\n",
        "                    self.ham_words[word] += 1\n",
        "                    self.total_ham_words += 1\n",
        "                    self.all_words.add(word) # 전체 단어 집합에 추가\n",
        "\n",
        "        self.vocab_size = len(self.all_words) # 학습 데이터의 전체 단어 집합 크기 사용\n",
        "        total_emails = self.spam_count + self.ham_count\n",
        "        self.prior_spam = self.spam_count / total_emails\n",
        "        self.prior_ham = self.ham_count / total_emails\n",
        "\n",
        "    def calculate_likelihood(self, word, class_type):\n",
        "        # 라플라스 스무딩 적용: 알려지지 않은 단어에 대해 0 확률이 되는 것을 방지\n",
        "        if class_type == 'spam':\n",
        "            # 학습 데이터의 전체 단어 집합 크기를 사용하여 스무딩 적용\n",
        "            return (self.spam_words.get(word, 0) + 1) / (self.total_spam_words + self.vocab_size)\n",
        "        else: # ham\n",
        "             # 학습 데이터의 전체 단어 집합 크기를 사용하여 스무딩 적용\n",
        "            return (self.ham_words.get(word, 0) + 1) / (self.total_ham_words + self.vocab_size)\n",
        "\n",
        "    def predict(self, message_tokens):\n",
        "        # 로그 확률 계산 시 초기값을 priors의 로그값으로 설정\n",
        "        prob_spam = np.log(self.prior_spam)\n",
        "        prob_ham = np.log(self.prior_ham)\n",
        "\n",
        "        for word in message_tokens:\n",
        "            # 알려지지 않은 단어에 대한 처리 (스무딩으로 이미 처리됨)\n",
        "            # 각 단어에 대한 로그 가능도를 더함\n",
        "            prob_spam += np.log(self.calculate_likelihood(word, 'spam'))\n",
        "            prob_ham += np.log(self.calculate_likelihood(word, 'ham'))\n",
        "\n",
        "        return 1 if prob_spam > prob_ham else 0, prob_spam, prob_ham\n",
        "\n",
        "# 데이터 분리 (훈련 세트와 테스트 세트)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['processed_message'], df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 분류기 인스턴스 생성 및 학습\n",
        "classifier = NaiveBayesClassifier()\n",
        "classifier.train(X_train.tolist(), y_train.tolist())\n",
        "\n",
        "print(f\"훈련된 스팸 메일 수: {classifier.spam_count}\")\n",
        "print(f\"훈련된 정상 메일 수: {classifier.ham_count}\")\n",
        "print(f\"총 단어 사전 크기 (고유한 단어 수 - 학습 데이터 기반): {classifier.vocab_size}\")"
      ],
      "metadata": {
        "id": "4aRnbDXgUroS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d604a51-dc1b-4f6d-e6c6-d9c28526da32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. 나이브 베이즈 분류기 구현 ---\n",
            "훈련된 스팸 메일 수: 35158\n",
            "훈련된 정상 메일 수: 31600\n",
            "총 단어 사전 크기 (고유한 단어 수 - 학습 데이터 기반): 250673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-4. 성능 평가\n",
        "**설명**: 학습된 나이브 베이즈 분류 모델의 성능을 평가합니다. 일반적으로 분류 모델의 성능을 측정하는 데 사용되는 정확도, 정밀도, 재현율, F1 점수를 계산합니다.\n",
        "\n",
        "**생성형 AI 활용 내역**:\n",
        "\n",
        "* scikit-learn 활용: sklearn.metrics 모듈에서 제공하는 accuracy_score, precision_score, recall_score, f1_score 함수를 사용하여 실제 레이블과 모델의 예측 결과를 비교하여 각 성능 지표를 계산하는 방법에 대한 구체적인 사용법을 안내받았습니다.\n",
        "\n",
        "\n",
        "**구현 방법**:\n",
        "\n",
        "* 테스트 세트(X_test)의 각 이메일 메시지에 대해 classifier.predict() 함수를 호출하여 예측 결과를 얻습니다.\n",
        "* 얻어진 예측 결과들을 y_pred 리스트에 저장합니다.\n",
        "* sklearn.metrics 모듈의 accuracy_score, precision_score, recall_score, f1_score 함수를 사용하여 실제 레이블(y_test)과 예측 결과(y_pred)를 비교하여 각 성능 지표를 계산합니다.\n",
        "* 계산된 성능 지표 값을 출력합니다."
      ],
      "metadata": {
        "id": "U85aKTvpYXsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- 4. 성능 평가 ---\n",
        "print(\"\\n--- 4. 성능 평가 ---\")\n",
        "\n",
        "y_pred = []\n",
        "# 테스트 세트의 메시지 토큰에 대해 예측 수행\n",
        "for msg_tokens in X_test.tolist():\n",
        "    pred, _, _ = classifier.predict(msg_tokens)\n",
        "    y_pred.append(pred)\n",
        "\n",
        "# 성능 지표 계산\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy (정확도): {accuracy:.4f}\")\n",
        "print(f\"Precision (정밀도): {precision:.4f}\")\n",
        "print(f\"Recall (재현율): {recall:.4f}\")\n",
        "print(f\"F1-Score (F1 점수): {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmLtu9jeX9ci",
        "outputId": "b2abaa23-a164-4b1a-f390-d30894a38a7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 4. 성능 평가 ---\n",
            "Accuracy (정확도): 0.9774\n",
            "Precision (정밀도): 0.9895\n",
            "Recall (재현율): 0.9671\n",
            "F1-Score (F1 점수): 0.9782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-5. 모델 해석 및 시각화\n",
        "**설명**: 이 부분에서는 모델의 예측 결과를 사용자가 이해하기 쉽도록 시각화하는 기능을 구현합니다. 특정 이메일이 스팸 또는 정상으로 분류된 이유를 설명하기 위해, 해당 이메일 내의 단어들이 스팸/정상 분류에 얼마나 기여했는지를 색상으로 표현하여 보여줍니다. 또한 사용자가 직접 이메일 내용을 입력하여 실시간으로 분류 및 시각화 결과를 확인할 수 있는 인터페이스를 제공합니다.\n",
        "\n",
        "**생성형 AI 활용 내역**:\n",
        "\n",
        "* 단어 기여도 계산 로직: 특정 임계값을 설정하여 \"강하게 기여하는\" 단어를 구분하는 아이디어를 얻었습니다.\n",
        "* HTML 기반 시각화: Jupyter/Colab 환경에서 텍스트의 특정 부분을 색상이나 굵은 글씨로 강조하여 시각화하는 방법으로 HTML 태그를 활용하는 것을 제안받았고, IPython.display.HTML을 사용하여 이를 구현하는 방법을 안내받았습니다.\n",
        "* 원문 텍스트와의 매핑: 전처리된 토큰과 원문 텍스트의 단어를 매핑하여 시각화할 때 원문의 단어를 그대로 유지하면서 색상만 적용하는 방법에 대한 구현 전략을 수립하는 데 도움을 받았습니다.\n",
        "* 테스트 예시 구성: 모델 시각화 기능이 정상적으로 작동하는지 확인하기 위한 스팸 및 정상 이메일 예시를 구성하는 아이디어를 얻었습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**구현 방법**:\n",
        "\n",
        "* visualize_email_classification 함수를 정의합니다. 이 함수는 나이브 베이즈 분류기 객체와 사용자가 입력한 이메일 텍스트를 인자로 받습니다.\n",
        " * 입력된 이메일 텍스트를 전처리 함수를 사용하여 처리하고, 모델의 predict 메서드를 호출하여 분류 결과와 스팸/정상 로그 확률을 얻습니다.\n",
        " *분류 결과를 사용자에게 출력합니다.\n",
        " *원본 이메일 텍스트를 단어 단위로 분리하고, 각 단어가 학습 데이터의 단어 사전에 포함되는지 확인합니다.\n",
        " *사전에 포함된 단어에 대해 스팸 및 정상 클래스에서의 로그 가능도를 계산합니다.\n",
        " *계산된 로그 가능도 차이가 특정 임계값(예: np.log(1.5)) 이상이면 해당 단어가 해당 클래스 분류에 강하게 기여한다고 판단하여 색상(스팸: 빨간색, 정상: 파란색)과 굵은 글씨로 강조하는 HTML 코드를 생성합니다.\n",
        " *강조된 단어들과 일반 단어들을 합쳐 HTML 형태로 변환하고, IPython.display.display(HTML(...))를 사용하여 Jupyter/Colab 환경에서 HTML 내용을 렌더링하여 시각화 결과를 보여줍니다.\n",
        "* 미리 정의된 여러 예시 이메일에 대해 visualize_email_classification 함수를 호출하여 시각화 결과를 테스트합니다.\n",
        "* 마지막으로, 사용자가 직접 이메일 내용을 입력하고 '끝' 또는 'exit'을 입력하기 전까지 분류 및 시각화를 반복하는 사용자 입력 루프를 구현합니다."
      ],
      "metadata": {
        "id": "U4WmR7lwZi1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- 5. 모델 해석 및 시각화 함수 ---\n",
        "print(\"\\n--- 5. 모델 해석 및 시각화 ---\")\n",
        "\n",
        "def visualize_email_classification(classifier_obj, email_text_input):\n",
        "    \"\"\"\n",
        "    사용자 입력 이메일을 분류하고, 분류에 기여한 키워드를 시각화하는 함수.\n",
        "    \"\"\"\n",
        "    print(f\"\\n입력하신 이메일 내용:\\n{email_text_input}\\n\")\n",
        "\n",
        "    # 원본 텍스트를 기준으로 토큰화 (시각화를 위해)\n",
        "    original_tokens = word_tokenize(str(email_text_input).lower())\n",
        "    # 전처리된 토큰 (모델 예측에 사용)\n",
        "    processed_tokens = preprocess_text(email_text_input)\n",
        "\n",
        "    if not processed_tokens:\n",
        "        print(\"전처리된 메시지에 분류 가능한 단어가 없어 분류할 수 없습니다. 이메일 내용을 확인해주세요.\")\n",
        "        # 시각화는 건너뛰지만, 원본 텍스트는 표시\n",
        "        display(HTML(\" \".join(original_tokens)))\n",
        "        return\n",
        "\n",
        "    # 모델 예측 수행\n",
        "    prediction, prob_spam, prob_ham = classifier_obj.predict(processed_tokens)\n",
        "    classification_result = \"스팸\" if prediction == 1 else \"정상\"\n",
        "\n",
        "    print(f\"이 이메일은 '{classification_result}' 메일로 분류되었습니다.\")\n",
        "    print(f\"스팸 로그 확률: {prob_spam:.2f}\")\n",
        "    print(f\"정상 로그 확률: {prob_ham:.2f}\\n\")\n",
        "\n",
        "    highlighted_text_parts = []\n",
        "    # 원본 토큰을 순회하며 시각화 적용\n",
        "    for word in original_tokens:\n",
        "        # 시각화에 사용할 단어는 특수문자 제거 후 소문자 변환하여 처리\n",
        "        cleaned_word = re.sub(r'[^a-z]', '', word).lower()\n",
        "\n",
        "        # 전처리 후 남은 단어에 대해서만 가능도 계산 및 색상 적용\n",
        "        if cleaned_word and cleaned_word in classifier_obj.all_words: # 단어가 비어있지 않고 학습 데이터의 전체 단어 집합에 포함된 경우만 고려\n",
        "            # 로그 가능도 계산\n",
        "            spam_likelihood_log = np.log(classifier_obj.calculate_likelihood(cleaned_word, 'spam'))\n",
        "            ham_likelihood_log = np.log(classifier_obj.calculate_likelihood(cleaned_word, 'ham'))\n",
        "\n",
        "            # 로그 가능도 차이를 기준으로 색상 결정\n",
        "            # 차이가 클수록 해당 클래스에 강하게 기여한다고 판단\n",
        "            # np.log(1.5)는 임계값으로, 이 값보다 차이가 커야 강한 기여로 판단\n",
        "            log_likelihood_ratio = spam_likelihood_log - ham_likelihood_log\n",
        "\n",
        "            if log_likelihood_ratio > np.log(1.5): # 스팸에 강하게 기여\n",
        "                highlighted_text_parts.append(f'<span style=\"color:red; font-weight:bold;\">{word}</span>')\n",
        "            elif log_likelihood_ratio < -np.log(1.5): # 정상에 강하게 기여\n",
        "                 highlighted_text_parts.append(f'<span style=\"color:blue; font-weight:bold;\">{word}</span>')\n",
        "            else: # 큰 차이 없는 경우\n",
        "                highlighted_text_parts.append(word)\n",
        "        else: # 학습 데이터에 없는 단어는 색상 적용 없이 추가\n",
        "            highlighted_text_parts.append(word)\n",
        "\n",
        "    print(\"이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\")\n",
        "    display(HTML(\" \".join(highlighted_text_parts)))\n",
        "\n",
        "# %%\n",
        "# --- 테스트 이메일 입력 및 시각화 예시 ---\n",
        "print(\"\\n--- 5. 모델 해석 및 시각화 (테스트 이메일 입력) ---\")\n",
        "\n",
        "# 정상 메일 (Ham) 예시\n",
        "user_input_email_1 = \"Hello, I've attached the meeting minutes. Let's discuss them again during the meeting next Monday at 10 AM. Please feel free to contact me if you have any questions. Have a great day!\"\n",
        "visualize_email_classification(classifier, user_input_email_1)\n",
        "\n",
        "user_input_email_2 = \"Long time no see! How have you been? Are you free this weekend for lunch? I'm looking forward to hearing back from you.\"\n",
        "visualize_email_classification(classifier, user_input_email_2)\n",
        "\n",
        "# 스팸 메일 (Spam) 예시\n",
        "user_input_email_3 = \"Congratulations! You've won a chance to become a millionaire! Click this link now to claim your free prize! This offer is valid for a limited time only!\"\n",
        "visualize_email_classification(classifier, user_input_email_3)\n",
        "\n",
        "user_input_email_4 = \"URGENT NOTICE: Your account has been compromised! Immediately change your password and strengthen your account security. Access via the email link provided.\"\n",
        "visualize_email_classification(classifier, user_input_email_4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "vEPfnLJfZi1h",
        "outputId": "282527b3-d96c-4561-bae2-858cfea46e33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 5. 모델 해석 및 시각화 ---\n",
            "\n",
            "--- 5. 모델 해석 및 시각화 (테스트 이메일 입력) ---\n",
            "\n",
            "입력하신 이메일 내용:\n",
            "Hello, I've attached the meeting minutes. Let's discuss them again during the meeting next Monday at 10 AM. Please feel free to contact me if you have any questions. Have a great day!\n",
            "\n",
            "이 이메일은 '정상' 메일로 분류되었습니다.\n",
            "스팸 로그 확률: -137.21\n",
            "정상 로그 확률: -131.29\n",
            "\n",
            "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "hello , i 've <span style=\"color:blue; font-weight:bold;\">attached</span> the <span style=\"color:blue; font-weight:bold;\">meeting</span> <span style=\"color:red; font-weight:bold;\">minutes</span> . let 's <span style=\"color:blue; font-weight:bold;\">discuss</span> them again during the <span style=\"color:blue; font-weight:bold;\">meeting</span> next <span style=\"color:blue; font-weight:bold;\">monday</span> at 10 am . please <span style=\"color:red; font-weight:bold;\">feel</span> free to <span style=\"color:red; font-weight:bold;\">contact</span> me if you have any <span style=\"color:blue; font-weight:bold;\">questions</span> . have a <span style=\"color:red; font-weight:bold;\">great</span> <span style=\"color:red; font-weight:bold;\">day</span> !"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "입력하신 이메일 내용:\n",
            "Long time no see! How have you been? Are you free this weekend for lunch? I'm looking forward to hearing back from you.\n",
            "\n",
            "이 이메일은 '스팸' 메일로 분류되었습니다.\n",
            "스팸 로그 확률: -85.76\n",
            "정상 로그 확률: -86.32\n",
            "\n",
            "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "long time no <span style=\"color:red; font-weight:bold;\">see</span> ! how have you been ? are you free this <span style=\"color:blue; font-weight:bold;\">weekend</span> for <span style=\"color:blue; font-weight:bold;\">lunch</span> ? i 'm <span style=\"color:red; font-weight:bold;\">looking</span> <span style=\"color:red; font-weight:bold;\">forward</span> to <span style=\"color:red; font-weight:bold;\">hearing</span> back from you ."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "입력하신 이메일 내용:\n",
            "Congratulations! You've won a chance to become a millionaire! Click this link now to claim your free prize! This offer is valid for a limited time only!\n",
            "\n",
            "이 이메일은 '스팸' 메일로 분류되었습니다.\n",
            "스팸 로그 확률: -112.47\n",
            "정상 로그 확률: -125.95\n",
            "\n",
            "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color:red; font-weight:bold;\">congratulations</span> ! you 've won a <span style=\"color:red; font-weight:bold;\">chance</span> to <span style=\"color:red; font-weight:bold;\">become</span> a <span style=\"color:red; font-weight:bold;\">millionaire</span> ! <span style=\"color:red; font-weight:bold;\">click</span> this <span style=\"color:red; font-weight:bold;\">link</span> now to <span style=\"color:red; font-weight:bold;\">claim</span> your free <span style=\"color:red; font-weight:bold;\">prize</span> ! this <span style=\"color:red; font-weight:bold;\">offer</span> is valid for a <span style=\"color:red; font-weight:bold;\">limited</span> time only !"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "입력하신 이메일 내용:\n",
            "URGENT NOTICE: Your account has been compromised! Immediately change your password and strengthen your account security. Access via the email link provided.\n",
            "\n",
            "이 이메일은 '스팸' 메일로 분류되었습니다.\n",
            "스팸 로그 확률: -125.76\n",
            "정상 로그 확률: -131.49\n",
            "\n",
            "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color:red; font-weight:bold;\">urgent</span> <span style=\"color:red; font-weight:bold;\">notice</span> : your <span style=\"color:red; font-weight:bold;\">account</span> has been <span style=\"color:red; font-weight:bold;\">compromised</span> ! <span style=\"color:red; font-weight:bold;\">immediately</span> <span style=\"color:blue; font-weight:bold;\">change</span> your <span style=\"color:blue; font-weight:bold;\">password</span> and strengthen your <span style=\"color:red; font-weight:bold;\">account</span> security . <span style=\"color:blue; font-weight:bold;\">access</span> via the email <span style=\"color:red; font-weight:bold;\">link</span> provided ."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-6. 사용자 입력\n",
        "**설명**: 사용자가 직접 이메일 내용을 입력하여 실시간으로 분류 및 시각화 결과를 확인할 수 있는 인터페이스를 제공합니다.\n",
        "\n",
        "**생성형 AI 활용 내역**:\n",
        "\n",
        "* 실행 결과 구분: 반복되는 사용자 입력과 그 결과를 명확하게 구분하기 위해 구분선(-*50)과 같은 시각적인 요소를 추가하는 아이디어를 얻었습니다.\n",
        "\n",
        "**구현 방법**:\n",
        "* 사용자가 직접 이메일 내용을 입력하고 '끝' 또는 'exit'을 입력하기 전까지 분류 및 시각화를 반복하는 사용자 입력 루프를 구현합니다."
      ],
      "metadata": {
        "id": "m1nMWIhcbHI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- 사용자 입력 루프 ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"       이메일 스팸 분류기 - 사용자 입력 모드\")\n",
        "print(\"=\"*50)\n",
        "print(\"이메일 내용을 입력해주세요. (입력을 마치려면 '끝' 또는 'exit'을 입력하세요)\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"이메일 내용 입력: \")\n",
        "    if user_input.lower() in ['끝', 'exit']:\n",
        "        print(\"프로그램을 종료합니다.\")\n",
        "        break\n",
        "\n",
        "    # 입력된 이메일 내용을 시각화 함수에 전달\n",
        "    visualize_email_classification(classifier, user_input)\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0RtTx9o3bEbY",
        "outputId": "7b44eefe-c9a4-4716-b7dc-7b09f864b54c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "       이메일 스팸 분류기 - 사용자 입력 모드\n",
            "==================================================\n",
            "이메일 내용을 입력해주세요. (입력을 마치려면 '끝' 또는 'exit'을 입력하세요)\n",
            "\n",
            "이메일 내용 입력: Hi Team Lead Kim, the materials you requested are almost ready. I should be able to send them over by the end of today. Please review and provide your feedback.\n",
            "\n",
            "입력하신 이메일 내용:\n",
            "Hi Team Lead Kim, the materials you requested are almost ready. I should be able to send them over by the end of today. Please review and provide your feedback.\n",
            "\n",
            "이 이메일은 '정상' 메일로 분류되었습니다.\n",
            "스팸 로그 확률: -136.10\n",
            "정상 로그 확률: -130.54\n",
            "\n",
            "이메일 내용 내 키워드 기여도 시각화 (빨간색: 스팸 기여, 파란색: 정상 기여):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color:blue; font-weight:bold;\">hi</span> team lead <span style=\"color:blue; font-weight:bold;\">kim</span> , the materials you <span style=\"color:blue; font-weight:bold;\">requested</span> are <span style=\"color:red; font-weight:bold;\">almost</span> <span style=\"color:red; font-weight:bold;\">ready</span> . i should be able to send them over by the end of today . please <span style=\"color:blue; font-weight:bold;\">review</span> and <span style=\"color:blue; font-weight:bold;\">provide</span> your <span style=\"color:blue; font-weight:bold;\">feedback</span> ."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "이메일 내용 입력: 끝\n",
            "프로그램을 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.생성형 AI 질문 리스트\n",
        "* 환경 설정 및 자연어 처리 준비:\n",
        " * NLTK 라이브러리 설치 및 텍스트 처리에 필요한 데이터(punkt, stopwords, wordnet 등)를 다운로드하고 설정하는 방법에 대한 안내\n",
        "* 데이터 로딩 및 환경 설정:\n",
        " * Kaggle 데이터셋을 Google Colab 환경에서 kagglehub를 사용하여 다운로드하고 pandas DataFrame으로 로드하는 방법\n",
        " * 발생 가능한 파일 경로 문제, 다양한 인코딩(UTF-8, ISO-8859-1 등)으로 CSV 파일을 로드하여 디코딩 오류를 해결하는 방법\n",
        " * Kaggle 인증이 필요한 경우 (Colab 메뉴를 통한 API 토큰 설정 등)에 대한 가이드라인\n",
        "* 텍스트 전처리 파이프라인 구축:\n",
        " * 이메일 텍스트 데이터에 적용할 표준적인 전처리 단계(소문자 변환, 특수문자 제거, 토큰화, 불용어 제거) 구성\n",
        " * 텍스트에서 특수문자 및 숫자를 효과적으로 제거하기 위한 정규 표현식 패턴 구성 및 사용법\n",
        " * nltk 라이브러리의 word_tokenize를 이용한 토큰화 및 stopwords를 이용한 불용어 제거 방법, 길이가 짧은 단어 제거 방법\n",
        " *전처리 함수를 DataFrame 컬럼에 효율적으로 적용하기 위한 pandas.apply 사용법\n",
        "* 나이브 베이즈 분류기 클래스 구현:\n",
        " * 학습 데이터에 나타나지 않은 단어의 확률이 0이 되는 것을 방지하는 라플라스 스무딩(Add-1 smoothing)의 개념 및 파이썬 코드로 구현하는 방법 (학습 데이터의 전체 단어 집합 크기 활용 포함)\n",
        "* 모델 학습 및 데이터 분할:\n",
        " * sklearn.model_selection.train_test_split을 사용하여 데이터를 훈련 세트와 테스트 세트로 분리하는 방법\n",
        "* 모델 성능 평가:\n",
        " * sklearn.metrics 모듈의 accuracy_score, precision_score, recall_score, f1_score 함수를 사용하여 성능 지표를 계산하는 구체적인 방법\n",
        "* 모델 해석 및 시각화 기법:\n",
        " * 특정 단어가 스팸 또는 정상 분류에 얼마나 기여하는지를 나타내는 지표(로그 가능도 차이) 계산 로직 설계 및 임계값 설정 아이디어\n",
        " * 전처리된 토큰과 원문 텍스트의 단어를 매핑하여 원문 형태를 유지하며 시각화하는 방법\n",
        " * 모델 시각화 기능 테스트를 위한 스팸 및 정상 이메일 예시 구성 아이디어\n",
        "* 사용자 입력 인터페이스:\n",
        " * 반복되는 사용자 입력과 결과를 구분하기 위한 시각적 요소(구분선 등) 추가 아이디어"
      ],
      "metadata": {
        "id": "6PEef03SjnkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.소감\n",
        "자연어를 다루는 프로젝트는 처음이라 어려움이 많았습니다. 특히 데이터 인코딩 문제나 NLTK 데이터 로딩 오류 같은 예상치 못한 문제들이 발생했을 때 당황하기도 했습니다. 하지만 오류 해결 과정이나 복잡한 구현 코드를 짤 때 생성형 AI의 도움을 받아 많은 부분 해결할 수 있었습니다. 처음에는 단순히 단어를 나누고 확률을 계산하는 간단한 개념이라 생각했던 나이브 베이즈 모델이 실제 라플라스 스무딩 적용이나 로그 확률 계산 등 생각보다 복잡한 과정으로 구현된다는 것을 배우며 개념에 대한 이해가 깊어졌습니다. 최종적으로 구현된 분류 모델의 성능과 시각화 기능도 만족스러웠습니다."
      ],
      "metadata": {
        "id": "BOAJT2GBUsFG"
      }
    }
  ]
}